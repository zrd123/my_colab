{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Ivd6NGm8WgS",
        "bGLAmL6RLrsG"
      ],
      "mount_file_id": "16cOATm4VS-AS88kL1f5ulybQLiOoCvyS",
      "authorship_tag": "ABX9TyML0L1dFI6PwWTyuwHrzx2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrd123/my_colab/blob/master/code/SSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 多尺度锚框"
      ],
      "metadata": {
        "id": "-Ivd6NGm8WgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l"
      ],
      "metadata": {
        "id": "tH3ToOfRMyTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from d2l import torch as d2l\n",
        "\n",
        "img = d2l.plt.imread('./drive/MyDrive/img/catdog.jpg')\n",
        "h, w = img.shape[:2]\n",
        "h, w"
      ],
      "metadata": {
        "id": "SDt3b43T8WIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_anchors(fmap_w, fmap_h, s):\n",
        "    d2l.set_figsize()\n",
        "    # 前两个维度上的值不影响输出\n",
        "    fmap = torch.zeros((1, 10, fmap_h, fmap_w))\n",
        "    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n",
        "    bbox_scale = torch.tensor((w, h, w, h))\n",
        "    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n",
        "                    anchors[0] * bbox_scale)"
      ],
      "metadata": {
        "id": "UmKJe92U8hwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fYt-CP8xo21K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " display_anchors(fmap_w=4, fmap_h=4, s=[0.15])"
      ],
      "metadata": {
        "id": "D-kow9z28oBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_anchors(fmap_w=1, fmap_h=1, s=[0.8])"
      ],
      "metadata": {
        "id": "LTjL9Swy8hnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSD"
      ],
      "metadata": {
        "id": "bGLAmL6RLrsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l"
      ],
      "metadata": {
        "id": "wmLRF4RYLxRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIogEAMbLnKb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
        "    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n",
        "                     kernel_size=3, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_predictor(num_inputs, num_anchors):\n",
        "    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)"
      ],
      "metadata": {
        "id": "29SPPMvCLvL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, block):\n",
        "    return block(x)\n",
        "# 分别卷积类别 和锚框的偏移量\n",
        "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
        "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
        "Y1.shape, Y2.shape"
      ],
      "metadata": {
        "id": "IH0SG81hMBWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_pred(pred):\n",
        "    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
        "\n",
        "def concat_preds(preds):\n",
        "    return torch.cat([flatten_pred(p) for p in preds], dim=1)"
      ],
      "metadata": {
        "id": "ic8uWp13MBTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将两个特征图进行拼接\n",
        "concat_preds([Y1, Y2]).shape"
      ],
      "metadata": {
        "id": "b8toNbcXMBQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tqCdl8GDGQxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def down_sample_blk(in_channels, out_channels):\n",
        "    blk = []\n",
        "    for _ in range(2):\n",
        "        blk.append(nn.Conv2d(in_channels, out_channels,\n",
        "                             kernel_size=3, padding=1))\n",
        "        blk.append(nn.BatchNorm2d(out_channels))\n",
        "        blk.append(nn.ReLU())\n",
        "        in_channels = out_channels\n",
        "    blk.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*blk)"
      ],
      "metadata": {
        "id": "rbO017KnMAi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape"
      ],
      "metadata": {
        "id": "kJNf8Oo1MAfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_net():\n",
        "    blk = []\n",
        "    num_filters = [3, 16, 32, 64]\n",
        "    for i in range(len(num_filters) - 1):\n",
        "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
        "    return nn.Sequential(*blk)\n",
        "\n",
        "# 3*(1+(2*3)) = 21\n",
        "forward(torch.zeros((2, 3, 256, 256)), base_net()).shape"
      ],
      "metadata": {
        "id": "zOWgf9JbMJRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 + 7 + 7 + 7 + 1\n",
        "def get_blk(i):\n",
        "    if i == 0:\n",
        "        blk = base_net()\n",
        "    elif i == 1:\n",
        "        blk = down_sample_blk(64, 128)\n",
        "    elif i == 4:\n",
        "        blk = nn.AdaptiveMaxPool2d((1,1))\n",
        "    else:\n",
        "        blk = down_sample_blk(128, 128)\n",
        "    return blk"
      ],
      "metadata": {
        "id": "FpiM81mXMJPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
        "    Y = blk(X)\n",
        "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
        "    cls_preds = cls_predictor(Y)\n",
        "    bbox_preds = bbox_predictor(Y)\n",
        "    return (Y, anchors, cls_preds, bbox_preds)"
      ],
      "metadata": {
        "id": "BJupT8SJMJMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型超参数"
      ],
      "metadata": {
        "id": "JLU_N0TAAaEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 锚框覆盖率,size锚框缩放比\n",
        "sizes = [[0.2, 0.272],\n",
        "         [0.37, 0.447],\n",
        "         [0.54, 0.619],\n",
        "         [0.71, 0.79],\n",
        "         [0.88, 0.961]]\n",
        "# radition锚框高宽比\n",
        "ratios = [[1, 2, 0.5]] * 5\n",
        "# 2 + 3 -1，以每个像素为中心生成4个锚框\n",
        "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
      ],
      "metadata": {
        "id": "wtBbkHdLMJKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinySSD(nn.Module):\n",
        "    def __init__(self, num_classes, **kwargs):\n",
        "        super(TinySSD, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        # 5个stage输出的channel数\n",
        "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
        "        for i in range(5):\n",
        "            # 即赋值语句self.blk_i=get_blk(i)\n",
        "            setattr(self, f'blk_{i}', get_blk(i))\n",
        "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
        "                                                    num_anchors, num_classes))\n",
        "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
        "                                                      num_anchors))\n",
        "\n",
        "    def forward(self, X):\n",
        "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
        "        for i in range(5):\n",
        "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
        "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
        "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
        "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
        "        anchors = torch.cat(anchors, dim=1)\n",
        "        cls_preds = concat_preds(cls_preds)\n",
        "        cls_preds = cls_preds.reshape(\n",
        "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
        "        bbox_preds = concat_preds(bbox_preds)\n",
        "        return anchors, cls_preds, bbox_preds"
      ],
      "metadata": {
        "id": "kh4rtANQMJG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = TinySSD(num_classes=1)\n",
        "# batch_size, channels, w, h\n",
        "X = torch.zeros((32, 3, 256, 256))\n",
        "anchors, cls_preds, bbox_preds = net(X)\n",
        "\n",
        "print('output anchors:', anchors.shape)\n",
        "print('output class preds:', cls_preds.shape)\n",
        "print('output bbox preds:', bbox_preds.shape)"
      ],
      "metadata": {
        "id": "ELqk1FNPMQQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_iter, _ = d2l.load_data_bananas(batch_size)\n",
        "\n",
        "device, net = d2l.try_gpu(), TinySSD(num_classes=1)\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "9AD2SDBCMQN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "UWl_gHtxYrT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 分类损失\n",
        "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "# 锚框损失，对错误不敏感\n",
        "bbox_loss = nn.L1Loss(reduction='none')\n",
        "\n",
        "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
        "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
        "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
        "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
        "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
        "                     bbox_labels * bbox_masks).mean(dim=1)\n",
        "    return cls + bbox"
      ],
      "metadata": {
        "id": "V751W17-MWVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cls_eval(cls_preds, cls_labels):\n",
        "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
        "    return float((cls_preds.argmax(dim=-1).type(\n",
        "        cls_labels.dtype) == cls_labels).sum())\n",
        "\n",
        "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
        "    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
      ],
      "metadata": {
        "id": "crY_FwvUMWST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, timer = 20, d2l.Timer()\n",
        "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                        legend=['class error', 'bbox mae'])\n",
        "net = net.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    # 训练精确度的和，训练精确度的和中的示例数\n",
        "    # 绝对误差的和，绝对误差的和中的示例数\n",
        "    metric = d2l.Accumulator(4)\n",
        "    net.train()\n",
        "    for features, target in train_iter:\n",
        "        timer.start()\n",
        "        trainer.zero_grad()\n",
        "        X, Y = features.to(device), target.to(device)\n",
        "        # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
        "        anchors, cls_preds, bbox_preds = net(X)\n",
        "        # 为每个锚框标注类别和偏移量\n",
        "        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n",
        "        # 根据类别和偏移量的预测和标注值计算损失函数\n",
        "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
        "                      bbox_masks)\n",
        "        l.mean().backward()\n",
        "        trainer.step()\n",
        "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\n",
        "                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
        "                   bbox_labels.numel())\n",
        "    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n",
        "    animator.add(epoch + 1, (cls_err, bbox_mae))\n",
        "print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\n",
        "print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\n",
        "      f'{str(device)}')"
      ],
      "metadata": {
        "id": "sWeXK9RDMWP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torchvision.io.read_image('./drive/MyDrive/img/banana.jpg').unsqueeze(0).float()\n",
        "img = X.squeeze(0).permute(1, 2, 0).long()"
      ],
      "metadata": {
        "id": "NJm_bFBWMWNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X):\n",
        "    net.eval()\n",
        "    anchors, cls_preds, bbox_preds = net(X.to(device))\n",
        "    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n",
        "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
        "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
        "    return output[0, idx]\n",
        "\n",
        "output = predict(X)"
      ],
      "metadata": {
        "id": "iBUoNLaAMWKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display(img, output, threshold):\n",
        "    d2l.set_figsize((5, 5))\n",
        "    fig = d2l.plt.imshow(img)\n",
        "    for row in output:\n",
        "        score = float(row[1])\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        h, w = img.shape[0:2]\n",
        "        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\n",
        "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
        "\n",
        "display(img, output.cpu(), threshold=0.9)"
      ],
      "metadata": {
        "id": "AmEuZsCRMdAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 语义分割"
      ],
      "metadata": {
        "id": "pFI-sFSFMUNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l"
      ],
      "metadata": {
        "id": "4-gYvSN9OXqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "0tBu4NGcj94Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n",
        "                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n",
        "\n",
        "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')"
      ],
      "metadata": {
        "id": "q17W0Ynvj9wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_voc_images(voc_dir, is_train=True):\n",
        "    \"\"\"读取所有VOC图像并标注\"\"\"\n",
        "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
        "                             'train.txt' if is_train else 'val.txt')\n",
        "    mode = torchvision.io.image.ImageReadMode.RGB\n",
        "    with open(txt_fname, 'r') as f:\n",
        "        images = f.read().split()\n",
        "    features, labels = [], []\n",
        "    for i, fname in enumerate(images):\n",
        "        features.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
        "        labels.append(torchvision.io.read_image(os.path.join(\n",
        "            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = read_voc_images(voc_dir, True)"
      ],
      "metadata": {
        "id": "9g63Yii7kHV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "imgs = train_features[0:n] + train_labels[0:n]\n",
        "imgs = [img.permute(1,2,0) for img in imgs]\n",
        "d2l.show_images(imgs, 2, n);"
      ],
      "metadata": {
        "id": "Yd8ZcdBPkHTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "                [0, 64, 128]]\n",
        "\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']"
      ],
      "metadata": {
        "id": "OE1M8mZfkHRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def voc_colormap2label():\n",
        "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
        "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
        "    for i, colormap in enumerate(VOC_COLORMAP):\n",
        "        colormap2label[\n",
        "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
        "    return colormap2label\n",
        "\n",
        "def voc_label_indices(colormap, colormap2label):\n",
        "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
        "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
        "           + colormap[:, :, 2])\n",
        "    return colormap2label[idx]"
      ],
      "metadata": {
        "id": "4V9e9BYrkHOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = voc_label_indices(train_labels[0], voc_colormap2label())\n",
        "y[105:115, 130:140], VOC_CLASSES[1]"
      ],
      "metadata": {
        "id": "3-s3V5uBkHL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def voc_rand_crop(feature, label, height, width):\n",
        "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
        "    rect = torchvision.transforms.RandomCrop.get_params(\n",
        "        feature, (height, width))\n",
        "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
        "    label = torchvision.transforms.functional.crop(label, *rect)\n",
        "    return feature, label\n",
        "\n",
        "imgs = []\n",
        "for _ in range(n):\n",
        "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
        "\n",
        "imgs = [img.permute(1, 2, 0) for img in imgs]\n",
        "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
      ],
      "metadata": {
        "id": "L-hBmSTZkHIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCSegDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"一个用于加载VOC数据集的自定义数据集\"\"\"\n",
        "\n",
        "    def __init__(self, is_train, crop_size, voc_dir):\n",
        "        self.transform = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.crop_size = crop_size\n",
        "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
        "        self.features = [self.normalize_image(feature)\n",
        "                         for feature in self.filter(features)]\n",
        "        self.labels = self.filter(labels)\n",
        "        self.colormap2label = voc_colormap2label()\n",
        "        print('read ' + str(len(self.features)) + ' examples')\n",
        "\n",
        "    def normalize_image(self, img):\n",
        "        return self.transform(img.float() / 255)\n",
        "\n",
        "    def filter(self, imgs):\n",
        "        return [img for img in imgs if (\n",
        "            img.shape[1] >= self.crop_size[0] and\n",
        "            img.shape[2] >= self.crop_size[1])]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
        "                                       *self.crop_size)\n",
        "        return (feature, voc_label_indices(label, self.colormap2label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)"
      ],
      "metadata": {
        "id": "KasPI-_qkeHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop_size = (320, 480)\n",
        "voc_train = VOCSegDataset(True, crop_size, voc_dir)\n",
        "voc_test = VOCSegDataset(False, crop_size, voc_dir)"
      ],
      "metadata": {
        "id": "DdjiqzcekhDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
        "                                    drop_last=True,\n",
        "                                    num_workers=d2l.get_dataloader_workers())\n",
        "for X, Y in train_iter:\n",
        "    print(X.shape)\n",
        "    print(Y.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "9ClDuEhxkeET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_voc(batch_size, crop_size):\n",
        "    \"\"\"加载VOC语义分割数据集\"\"\"\n",
        "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
        "        'VOCdevkit', 'VOC2012'))\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    train_iter = torch.utils.data.DataLoader(\n",
        "        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n",
        "        shuffle=True, drop_last=True, num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(\n",
        "        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n",
        "        drop_last=True, num_workers=num_workers)\n",
        "    return train_iter, test_iter"
      ],
      "metadata": {
        "id": "SVlDsUUzklYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aECAsH6zklVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}