{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "_bu-12lgR2Ty"
      ],
      "mount_file_id": "1USb_PbAGxuPnJRX8Qd5B1efLmWCphpst",
      "authorship_tag": "ABX9TyPPdhkSU9OYWVKr8uNRLlKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrd123/my_colab/blob/master/code/Transposed_convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 转置卷积"
      ],
      "metadata": {
        "id": "_bu-12lgR2Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l"
      ],
      "metadata": {
        "id": "GYeFXiDLubl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H1C0KoIZlEM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trans_conv(X, K):\n",
        "    h, w = K.shape\n",
        "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
        "    return Y"
      ],
      "metadata": {
        "id": "HVVs_kewZn2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "trans_conv(X, K), X, K"
      ],
      "metadata": {
        "id": "enqSkcI5ZtzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "metadata": {
        "id": "X-TRinfE74Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding=1,将输入的padding-1，因为转置卷积会默认进行kernel_size - 1的padding\n",
        "# 或者说将输出（卷积）结果去除 一个 padding\n",
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "metadata": {
        "id": "IkxPzn1LZtw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "metadata": {
        "id": "FQQyCVHA-fmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, padding = 1, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ],
      "metadata": {
        "id": "tnLTcwyjZtuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(size=(1, 10, 16, 16))\n",
        "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
        "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
        "tconv(conv(X)).shape == X.shape, tconv(conv(X))"
      ],
      "metadata": {
        "id": "m96U39glZtr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(9.0).reshape(3, 3)\n",
        "K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "Y = d2l.corr2d(X, K)\n",
        "Y"
      ],
      "metadata": {
        "id": "dVi4ZgdsZtpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel2matrix(K):\n",
        "    k, W = torch.zeros(5), torch.zeros((4, 9))\n",
        "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
        "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
        "    return W\n",
        "\n",
        "W = kernel2matrix(K)\n",
        "W"
      ],
      "metadata": {
        "id": "mXXi9yqdZ23b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算矩阵乘积\n",
        "Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2)"
      ],
      "metadata": {
        "id": "WCYLljD_Z203"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = trans_conv(Y, K)\n",
        "Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)"
      ],
      "metadata": {
        "id": "Glydh7xjZ2yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FNC"
      ],
      "metadata": {
        "id": "MuIkY8fYR6Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l==0.17.6"
      ],
      "metadata": {
        "id": "pxTXlWs1aY0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "\n",
        "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
        "list(pretrained_net.children())[-3:]"
      ],
      "metadata": {
        "id": "W1WLipjnZ2wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(*list(pretrained_net.children())[:-2])\n",
        "\n",
        "X = torch.rand(size=(1, 3, 320, 480))\n",
        "net(X).shape"
      ],
      "metadata": {
        "id": "2pg8S1xNZ2tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 21\n",
        "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
        "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
        "                                    kernel_size=64, padding=16, stride=32))"
      ],
      "metadata": {
        "id": "fRtsr0SvUO9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
        "    factor = (kernel_size + 1) // 2\n",
        "    if kernel_size % 2 == 1:\n",
        "        center = factor - 1\n",
        "    else:\n",
        "        center = factor - 0.5\n",
        "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
        "          torch.arange(kernel_size).reshape(1, -1))\n",
        "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
        "           (1 - torch.abs(og[1] - center) / factor)\n",
        "    weight = torch.zeros((in_channels, out_channels,\n",
        "                          kernel_size, kernel_size))\n",
        "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
        "    return weight"
      ],
      "metadata": {
        "id": "RBKCffSqZ2q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
        "                                bias=False)\n",
        "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));"
      ],
      "metadata": {
        "id": "Nw0p0G7BTc73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torchvision.transforms.ToTensor()(d2l.Image.open('./drive/MyDrive/img/catdog.jpg'))\n",
        "X = img.unsqueeze(0)\n",
        "Y = conv_trans(X)\n",
        "out_img = Y[0].permute(1, 2, 0).detach()"
      ],
      "metadata": {
        "id": "UaqaXBbrTc5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.set_figsize()\n",
        "print('input image shape:', img.permute(1, 2, 0).shape)\n",
        "d2l.plt.imshow(img.permute(1, 2, 0));\n",
        "print('output image shape:', out_img.shape)\n",
        "d2l.plt.imshow(out_img);"
      ],
      "metadata": {
        "id": "URC9MEP8Tc1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = bilinear_kernel(num_classes, num_classes, 64)\n",
        "net.transpose_conv.weight.data.copy_(W);"
      ],
      "metadata": {
        "id": "QTS9G1iTTsOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, crop_size = 32, (320, 480)\n",
        "train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
      ],
      "metadata": {
        "id": "7fiWCIoXTsKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(inputs, targets):\n",
        "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
        "\n",
        "num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
        "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
      ],
      "metadata": {
        "id": "Xo4AagxjTsFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(img):\n",
        "    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n",
        "    pred = net(X.to(devices[0])).argmax(dim=1)\n",
        "    return pred.reshape(pred.shape[1], pred.shape[2])"
      ],
      "metadata": {
        "id": "2BO6N8BRT1__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label2image(pred):\n",
        "    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n",
        "    X = pred.long()\n",
        "    return colormap[X, :]"
      ],
      "metadata": {
        "id": "HKz1r8ZqT19G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
        "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
        "n, imgs = 4, []\n",
        "for i in range(n):\n",
        "    crop_rect = (0, 0, 320, 480)\n",
        "    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n",
        "    pred = label2image(predict(X))\n",
        "    imgs += [X.permute(1,2,0), pred.cpu(),\n",
        "             torchvision.transforms.functional.crop(\n",
        "                 test_labels[i], *crop_rect).permute(1,2,0)]\n",
        "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
      ],
      "metadata": {
        "id": "obHphm9YT16T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 样式迁移"
      ],
      "metadata": {
        "id": "W-icXbc3o11i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install d2l"
      ],
      "metadata": {
        "id": "dVjFOUlBHbhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "d2l.set_figsize()\n",
        "content_img = d2l.Image.open('./drive/MyDrive/img/rainier.jpg')\n",
        "d2l.plt.imshow(content_img);"
      ],
      "metadata": {
        "id": "iM2vBCTco0md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_img = d2l.Image.open('./drive/MyDrive/img/autumn-oak.jpg')\n",
        "d2l.plt.imshow(style_img);"
      ],
      "metadata": {
        "id": "eJ8sl9Rkrgc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "def preprocess(img, image_shape):\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(image_shape),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
        "    return transforms(img).unsqueeze(0)\n",
        "\n",
        "def postprocess(img):\n",
        "    img = img[0].to(rgb_std.device)\n",
        "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
        "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"
      ],
      "metadata": {
        "id": "nHMl9W6ergaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_net = torchvision.models.vgg19(pretrained=True)"
      ],
      "metadata": {
        "id": "Gv-mpRcMrgYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "style_layers, content_layers = [0, 5, 10, 19, 28], [25]"
      ],
      "metadata": {
        "id": "DIQ7vlCvrgVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(*[pretrained_net.features[i] for i in\n",
        "                      range(max(content_layers + style_layers) + 1)])"
      ],
      "metadata": {
        "id": "ZFqYfHCArgSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(X, content_layers, style_layers):\n",
        "    contents = []\n",
        "    styles = []\n",
        "    for i in range(len(net)):\n",
        "        X = net[i](X)\n",
        "        if i in style_layers:\n",
        "            styles.append(X)\n",
        "        if i in content_layers:\n",
        "            contents.append(X)\n",
        "    return contents, styles"
      ],
      "metadata": {
        "id": "tqeCHYI8rgP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_contents(image_shape, device):\n",
        "    content_X = preprocess(content_img, image_shape).to(device)\n",
        "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
        "    return content_X, contents_Y\n",
        "\n",
        "def get_styles(image_shape, device):\n",
        "    style_X = preprocess(style_img, image_shape).to(device)\n",
        "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
        "    return style_X, styles_Y"
      ],
      "metadata": {
        "id": "zQ0YM56XrgNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(Y_hat, Y):\n",
        "    # 我们从动态计算梯度的树中分离目标：\n",
        "    # 这是一个规定的值，而不是一个变量。\n",
        "    return torch.square(Y_hat - Y.detach()).mean()"
      ],
      "metadata": {
        "id": "4ryOSCvWrgLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram(X):\n",
        "    num_channels, n = X.shape[1], X.numel() // X.shape[1]\n",
        "    X = X.reshape((num_channels, n))\n",
        "    return torch.matmul(X, X.T) / (num_channels * n)"
      ],
      "metadata": {
        "id": "9pY1YwL9ryv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def style_loss(Y_hat, gram_Y):\n",
        "    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()"
      ],
      "metadata": {
        "id": "bzlrfkITrytN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tv_loss(Y_hat):\n",
        "    return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
        "                  torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())"
      ],
      "metadata": {
        "id": "n5JhU4gNrgHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
        "\n",
        "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
        "    # 分别计算内容损失、风格损失和全变分损失\n",
        "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
        "        contents_Y_hat, contents_Y)]\n",
        "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
        "        styles_Y_hat, styles_Y_gram)]\n",
        "    tv_l = tv_loss(X) * tv_weight\n",
        "    # 对所有损失求和\n",
        "    l = sum(10 * styles_l + contents_l + [tv_l])\n",
        "    return contents_l, styles_l, tv_l, l"
      ],
      "metadata": {
        "id": "8I5ky-McrgEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SynthesizedImage(nn.Module):\n",
        "    def __init__(self, img_shape, **kwargs):\n",
        "        super(SynthesizedImage, self).__init__(**kwargs)\n",
        "        self.weight = nn.Parameter(torch.rand(*img_shape))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.weight"
      ],
      "metadata": {
        "id": "upFfduAyrgAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inits(X, device, lr, styles_Y):\n",
        "    gen_img = SynthesizedImage(X.shape).to(device)\n",
        "    gen_img.weight.data.copy_(X.data)\n",
        "    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
        "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
        "    return gen_img(), styles_Y_gram, trainer"
      ],
      "metadata": {
        "id": "q5CWF7Air8sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
        "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[10, num_epochs],\n",
        "                            legend=['content', 'style', 'TV'],\n",
        "                            ncols=2, figsize=(7, 2.5))\n",
        "    for epoch in range(num_epochs):\n",
        "        trainer.zero_grad()\n",
        "        contents_Y_hat, styles_Y_hat = extract_features(\n",
        "            X, content_layers, style_layers)\n",
        "        contents_l, styles_l, tv_l, l = compute_loss(\n",
        "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        scheduler.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            animator.axes[1].imshow(postprocess(X))\n",
        "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
        "                                     float(sum(styles_l)), float(tv_l)])\n",
        "    return X"
      ],
      "metadata": {
        "id": "-MpukMYqr8p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device, image_shape = d2l.try_gpu(), (300, 450)\n",
        "net = net.to(device)\n",
        "content_X, contents_Y = get_contents(image_shape, device)\n",
        "_, styles_Y = get_styles(image_shape, device)\n",
        "output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)"
      ],
      "metadata": {
        "id": "OnVDhOyDsAeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGenYcKosAbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_QGLSBBr8nA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}